{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90tetzst6hMs",
   "metadata": {
    "id": "90tetzst6hMs"
   },
   "source": [
    "# Music Composer Identification using Deep Learning\n",
    "\n",
    "The primary objective of this project is to develop a deep learning model that can predict the composer of a given musical score accurately. The project aims to accomplish this objective by using two deep learning techniques: Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN).\n",
    "\n",
    "## Project Team & Responsibilities:\n",
    "\n",
    "* **Dom:** Data Collection, Data Preprocessing (MIDI conversion, segmentation, augmentation), Feature Extraction (Piano Rolls for CNN, Sequential Features for LSTM).\n",
    "* **Santosh:** CNN Model Building, Training, Evaluation, Optimization.\n",
    "* **Jim:** LSTM Model Building, Training, Evaluation, Optimization.\n",
    "\n",
    "## Project Roadmap & Status:\n",
    "\n",
    "Here's a breakdown of our project phases and current status:\n",
    "\n",
    "1.  **Initial Setup & Data Download (COMPLETED by Jim):**\n",
    "    * Basic imports are set up.\n",
    "    * The `blanderbuss/midi-classic-music` dataset has been downloaded from Kaggle.\n",
    "    * *Status:* Ready for data processing.\n",
    "\n",
    "2.  **Data Preprocessing & Feature Extraction (COMPLETED by Dom):**\n",
    "    * **Goal:** Convert raw MIDI files into numerical features (Piano Rolls for CNNs, Sequential Features for LSTMs) and augment dataset.\n",
    "    * **Responsible:** Dom.\n",
    "    * *Current Status:* Completed / Needs implementation of the sections below.\n",
    "\n",
    "3.  **Model Building (NEXT STEP for Team):**\n",
    "    * **Goal:** Design CNN and LSTM model architectures.\n",
    "    * **Responsible:** Santosh (CNN), Jim (LSTM).\n",
    "    * *Dependencies:* Requires processed data from Phase 2.\n",
    "\n",
    "4.  **Model Training & Evaluation (AFTER Model Building):**\n",
    "    * **Goal:** Train the models and evaluate their performance using metrics like accuracy, precision, and recall.\n",
    "    * **Responsible:** Santosh (CNN), Jim (LSTM).\n",
    "    * *Dependencies:* Requires built models from Phase 3.\n",
    "\n",
    "5.  **Model Optimization (Post Training):**\n",
    "    * **Goal:** Fine-tune model hyperparameters to improve performance.\n",
    "    * **Responsible:** Santosh (CNN), Jim (LSTM) & Dom (Feature Engineering).\n",
    "    * *Dependencies:* Requires initial model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d11aba",
   "metadata": {
    "executionInfo": {
     "elapsed": 5596,
     "status": "ok",
     "timestamp": 1754862072736,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "c3d11aba"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader, TensorDataset, random_split\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import collections\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1348afbb",
   "metadata": {
    "id": "1348afbb"
   },
   "source": [
    "Data Collection\n",
    "The dataset contains the midi files of compositions from well-known classical composers like Bach, Beethoven, Chopin, and Mozart. The dataset has been labeled with the name of the composer for each score. Predictions are performed for only the below composers:\n",
    "\n",
    "1-Bach\n",
    "\n",
    "2-Beethoven\n",
    "\n",
    "3-Chopin\n",
    "\n",
    "4-Mozart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76171d1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2771,
     "status": "ok",
     "timestamp": 1754862075568,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "76171d1a",
    "outputId": "30a5636e-87c2-4dcb-cde7-e0ae32e49b04"
   },
   "outputs": [],
   "source": [
    "#%pip install kagglehub\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de884095",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3631,
     "status": "ok",
     "timestamp": 1754862079202,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "de884095",
    "outputId": "f3349e92-5be1-4e50-a7a7-b310f83f1554"
   },
   "outputs": [],
   "source": [
    "# # List all files in the dataset path\n",
    "# for root, dirs, files in os.walk(path):\n",
    "#     for file in files:\n",
    "#         #print(os.path.join(root, file))\n",
    "#         # Check if the file is a MIDI file and contains 'bach' in its name.\n",
    "#         # There are other composers that need to be processed too.\n",
    "#         if (file.endswith('.mid') or file.endswith('.midi')) and 'bach' in file.lower():\n",
    "#             print(f\"Found MIDI file: {file}\")\n",
    "#             # Add file to Bach dataset processing logic here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a8eac3",
   "metadata": {
    "id": "b1a8eac3"
   },
   "source": [
    "Convert MIDI file to something useful for LSTM and CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oBmREYtE1Nwc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9907,
     "status": "ok",
     "timestamp": 1754862089113,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "oBmREYtE1Nwc",
    "outputId": "4413fac9-ad13-464b-9048-e68844c8ec47"
   },
   "outputs": [],
   "source": [
    "# I will place these here so they run after Kaggle download, as I encountered conflicts with the initial setup when adding above.\n",
    "# !pip install music21\n",
    "# !pip install pretty_midi\n",
    "#!pip install --upgrade numpy # Ensure I have a recent numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43d89e",
   "metadata": {
    "executionInfo": {
     "elapsed": 1354,
     "status": "ok",
     "timestamp": 1754862090469,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "3b43d89e"
   },
   "outputs": [],
   "source": [
    "import music21\n",
    "import pretty_midi\n",
    "# Ensure numpy is up-to-date\n",
    "import numpy as np  # Already imported, but good to have here for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6f3a3",
   "metadata": {
    "id": "fca6f3a3"
   },
   "source": [
    "Data Pre-processing: Convert the musical scores into a format suitable for deep learning models. This involves converting the musical scores into MIDI files and applying data augmentation techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85boMSuo1Uhi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1754862785764,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "85boMSuo1Uhi",
    "outputId": "885e0d4d-f060-4e59-e91b-7dcaf49f998f"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing and Feature Extraction\n",
    "HOME_DIR = Path.home()\n",
    "KAGGLE_DOWNLOAD_PATH = HOME_DIR / \".cache\" / \"kagglehub\" / \"datasets\" / \"blanderbuss\" / \"midi-classic-music\" / \"versions\" / \"1\"\n",
    "MIDI_DIR = str(KAGGLE_DOWNLOAD_PATH)\n",
    "\n",
    "OUTPUT_DIR = \"./content/processed_data/\"\n",
    "SEGMENT_DURATION_SECONDS = 5\n",
    "SAMPLES_PER_SECOND = 100\n",
    "\n",
    "PITCH_LOW = 21\n",
    "PITCH_HIGH = 108\n",
    "NUM_PITCHES = PITCH_HIGH - PITCH_LOW + 1\n",
    "\n",
    "AUGMENT_TRANSPOSITION_STEPS = [-3, -2, -1, 1, 2, 3]\n",
    "AUGMENT_TEMPO_SCALES = [0.9, 1.1]\n",
    "\n",
    "# Defines composers\n",
    "COMPOSERS = [\"Bach\", \"Beethoven\", \"Chopin\", \"Mozart\"]\n",
    "\n",
    "# Creates output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"MIDI data will be processed from: {MIDI_DIR}\")\n",
    "print(f\"Processed data will be saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "#DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 42  # Set a seed for reproducibility\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using CUDA GPU\")\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark=False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R-F6z-ax4dPw",
   "metadata": {
    "id": "R-F6z-ax4dPw"
   },
   "source": [
    "###Feature Extraction : Extracts features from the MIDI files, such as notes, chords, and tempo, using music analysis tools.\n",
    "\n",
    "Here, the preprocessed MIDI segments are converted into numerical representations. I've generated different formats for the CNN and LSTM models to leverage the strengths of each.\n",
    "\n",
    "* **For CNNs: The Piano Roll**\n",
    "    * **Purpose:** CNNs excel at recognizing visual patterns. A piano roll converts music into a 2D image (pitch vs. time), allowing the CNN to \"see\" and learn characteristic melodic shapes, harmonic voicings, and rhythmic patterns that define a composer's style.\n",
    "    * **Details:** The piano roll captures note activity (velocity) across a defined pitch range (MIDI 21-108) over time, sampled at 100 samples per second. All outputs are normalized to [0,1] and padded/truncated to a consistent shape.\n",
    "* **For LSTMs: Sequential Features (Chroma & Note Density)**\n",
    "    * **Purpose:** LSTMs are great tools for understanding temporal sequences. These features describe the harmonic content and musical activity at each point in time, allowing the LSTM to learn how a composer's musical ideas evolve.\n",
    "    * **Details:** Each time step in the sequence contains a 12-element Pitch Class Profile (Chroma) representing harmonic presence (e.g., C, C#, D) and a single value for overall note density/volume. These are also sampled at 100 samples per second and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZB8qbxWy2Xox",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1754862090578,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "ZB8qbxWy2Xox"
   },
   "outputs": [],
   "source": [
    "# Feature Extraction - midi_to_sequential_features (for LSTMs)\n",
    "# This function extracts time-series features like Pitch Class Profiles and note density from a MIDI segment for LSTMs\n",
    "\n",
    "def midi_to_sequential_features(midi_data_segment: pretty_midi.PrettyMIDI, duration: float,\n",
    "                                samples_per_second: int, pitch_low: int, pitch_high: int) -> np.ndarray:\n",
    "    if not midi_data_segment.instruments:\n",
    "        return None\n",
    "\n",
    "    num_target_time_steps = int(duration * samples_per_second)\n",
    "    num_features_per_timestep = 12 + 1 # Chroma + Note Density\n",
    "    sequential_features = np.zeros((num_target_time_steps, num_features_per_timestep), dtype=np.float32)\n",
    "\n",
    "    chroma_features = midi_data_segment.get_chroma(fs=samples_per_second).T\n",
    "    # print(\"Original chroma shape:\", chroma_features.shape)  # should be (12, T)\n",
    "    if chroma_features.shape[0] < num_target_time_steps:\n",
    "        padding_needed = num_target_time_steps - chroma_features.shape[0]\n",
    "        chroma_features = np.pad(chroma_features, ((0, padding_needed), (0, 0)), mode='constant')\n",
    "    elif chroma_features.shape[0] > num_target_time_steps:\n",
    "        chroma_features = chroma_features[:num_target_time_steps, :]\n",
    "\n",
    "    note_density = np.zeros(num_target_time_steps, dtype=np.float32)\n",
    "    for instrument in midi_data_segment.instruments:\n",
    "        for note in instrument.notes:\n",
    "            start_idx = int(note.start * samples_per_second)\n",
    "            end_idx = int(note.end * samples_per_second)\n",
    "            start_idx = max(0, min(start_idx, num_target_time_steps - 1))\n",
    "            end_idx = max(0, min(end_idx, num_target_time_steps - 1))\n",
    "            if end_idx >= start_idx:\n",
    "                note_density[start_idx:end_idx] += note.velocity\n",
    "\n",
    "    max_density = np.max(note_density)\n",
    "    if max_density > 0:\n",
    "        note_density /= max_density\n",
    "\n",
    "    sequential_features[:, :12] = chroma_features\n",
    "    sequential_features[:, 12] = note_density\n",
    "\n",
    "    return sequential_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ykozm5Z92Xs1",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1754862090594,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "ykozm5Z92Xs1"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "# Feature Extraction - midi_to_piano_roll (for CNNs)\n",
    "# This function converts a MIDI segment into a 2D image-like \"piano roll\" for CNNs.\n",
    "def is_piano(instrument: pretty_midi.Instrument) -> bool:\n",
    "    # Check program number (0-7 are all piano-related in General MIDI)\n",
    "    return not instrument.is_drum and 0 <= instrument.program <= 7\n",
    "\n",
    "def midi_to_piano_roll(midi_data_segment: pretty_midi.PrettyMIDI, duration: float,\n",
    "                        samples_per_second: int, pitch_low: int, pitch_high: int) -> Optional[np.ndarray]:\n",
    "    if not midi_data_segment.instruments:\n",
    "        return None\n",
    "    piano = None # Default instrument of acoustic piano, will be updated if a piano instrument is found\n",
    "    for instrument in midi_data_segment.instruments:\n",
    "        if is_piano(instrument):\n",
    "            piano = instrument\n",
    "    if (piano is None):\n",
    "        # print(\"No piano instrument found in MIDI segment.\")\n",
    "        return None\n",
    "\n",
    "    # Fix: Use 'times' parameter and slice the piano roll to get the desired pitch range\n",
    "    piano_roll = piano.get_piano_roll(fs=samples_per_second)\n",
    "\n",
    "    # Slice to get the desired pitch range (pitch_low to pitch_high)\n",
    "    piano_roll = piano_roll[pitch_low:pitch_high+1, :]\n",
    "    piano_roll = piano_roll / 127.0\n",
    "\n",
    "    num_target_time_steps = int(duration * samples_per_second)\n",
    "    num_pitches = pitch_high - pitch_low + 1  # Should be 88\n",
    "    current_time_steps = piano_roll.shape[1]\n",
    "\n",
    "    if current_time_steps < num_target_time_steps:\n",
    "        padding = np.zeros((num_pitches, num_target_time_steps - current_time_steps), dtype=np.float32)\n",
    "        piano_roll = np.hstack([piano_roll, padding])\n",
    "    elif current_time_steps > num_target_time_steps:\n",
    "        piano_roll = piano_roll[:, :num_target_time_steps]\n",
    "\n",
    "    return piano_roll.reshape(num_pitches, num_target_time_steps, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DhDQAVpr17wT",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1754862090614,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "DhDQAVpr17wT"
   },
   "outputs": [],
   "source": [
    "# Utility Function - create_pretty_midi_segment\n",
    "# This function extracts a specific time segment from a larger MIDI file.\n",
    "\n",
    "def create_pretty_midi_segment(full_midi_data: pretty_midi.PrettyMIDI, start_time: float, end_time: float) -> pretty_midi.PrettyMIDI:\n",
    "    segment_pm = pretty_midi.PrettyMIDI()\n",
    "    for instrument in full_midi_data.instruments:\n",
    "        new_instrument = pretty_midi.Instrument(program=instrument.program, is_drum=instrument.is_drum, name=instrument.name)\n",
    "        for note in instrument.notes:\n",
    "            if note.end > start_time and note.start < end_time:\n",
    "                new_note = pretty_midi.Note(\n",
    "                    velocity=note.velocity,\n",
    "                    pitch=note.pitch,\n",
    "                    start=max(0.0, note.start - start_time),\n",
    "                    end=min(end_time - start_time, note.end - start_time)\n",
    "                )\n",
    "                if new_note.end > new_note.start:\n",
    "                    new_instrument.notes.append(new_note)\n",
    "        if new_instrument.notes:\n",
    "            segment_pm.instruments.append(new_instrument)\n",
    "    return segment_pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dmwdendo2XvZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1754862090618,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "dmwdendo2XvZ"
   },
   "outputs": [],
   "source": [
    "# Utility Function - apply_augmentation\n",
    "# This function modifies a MIDI segment by transposing its pitch or scaling its tempo.\n",
    "\n",
    "def apply_augmentation(midi_data_segment: pretty_midi.PrettyMIDI, augmentation_type: str, value) -> pretty_midi.PrettyMIDI:\n",
    "    augmented_midi = pretty_midi.PrettyMIDI()\n",
    "    for instrument in midi_data_segment.instruments:\n",
    "        new_instrument = pretty_midi.Instrument(program=instrument.program, is_drum=instrument.is_drum, name=instrument.name)\n",
    "        for note in instrument.notes:\n",
    "            new_note = pretty_midi.Note(note.velocity, note.pitch, note.start, note.end)\n",
    "            new_instrument.notes.append(new_note)\n",
    "        augmented_midi.instruments.append(new_instrument)\n",
    "\n",
    "    if augmentation_type == 'transpose':\n",
    "        for instrument in augmented_midi.instruments:\n",
    "            for note in instrument.notes:\n",
    "                note.pitch = int(max(0, min(127, note.pitch + value)))\n",
    "    elif augmentation_type == 'tempo_scale':\n",
    "        scale_value = float(value)\n",
    "        for instrument in augmented_midi.instruments:\n",
    "            for note in instrument.notes:\n",
    "                note.start *= scale_value\n",
    "                note.end *= scale_value\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown augmentation type: {augmentation_type}\")\n",
    "    return augmented_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783bed0",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1754862090634,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "4783bed0"
   },
   "outputs": [],
   "source": [
    "def extract_segments_from_midi(midi_path, segment_duration=5.0, samples_per_second=100):\n",
    "    try:\n",
    "        full_midi = pretty_midi.PrettyMIDI(midi_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {midi_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    total_duration = full_midi.get_end_time()\n",
    "    segments = []\n",
    "\n",
    "    for start_time in np.arange(0, total_duration, segment_duration):\n",
    "        end_time = min(start_time + segment_duration, total_duration)\n",
    "\n",
    "        segment = pretty_midi.PrettyMIDI()\n",
    "        for instrument in full_midi.instruments:\n",
    "            new_instrument = pretty_midi.Instrument(program=instrument.program, is_drum=instrument.is_drum)\n",
    "            for note in instrument.notes:\n",
    "                if start_time <= note.start < end_time:\n",
    "                    new_note = pretty_midi.Note(\n",
    "                        velocity=note.velocity,\n",
    "                        pitch=note.pitch,\n",
    "                        start=note.start - start_time,\n",
    "                        end=min(note.end, end_time) - start_time\n",
    "                    )\n",
    "                    new_instrument.notes.append(new_note)\n",
    "            if new_instrument.notes:\n",
    "                segment.instruments.append(new_instrument)\n",
    "\n",
    "        # Only append segments with valid instruments\n",
    "        if segment.instruments:\n",
    "            segments.append(segment)\n",
    "\n",
    "    return segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J1_k_NN85H2c",
   "metadata": {
    "id": "J1_k_NN85H2c"
   },
   "source": [
    "### Data Processing Loop & Output Conclusion\n",
    "\n",
    "This section orchestrates the loading of MIDI files, segmenting them, applying all augmentations, extracting features, and finally saving the processed data.\n",
    "\n",
    "* **Process:** Iterates through each composer's MIDI files, segments them, applies both transposition and tempo scaling for each segment, and then generates both CNN and LSTM features.\n",
    "* **Output Data:** The processed features and corresponding labels are saved as `.pkl` files in the `/content/processed_data/` directory.\n",
    "\n",
    "---\n",
    "\n",
    "#### **The data is ready for model training!**\n",
    "\n",
    "* **For CNN Model (Santosh):**\n",
    "    * Load `features_cnn.pkl`.\n",
    "    * Expected input shape: `(num_segments, 88, 500, 1)` - (total samples, pitches, time steps, channels).\n",
    "* **For LSTM Model (Jim):**\n",
    "    * Load `features_lstm.pkl`.\n",
    "    * Expected input shape: `(num_segments, 500, 13)` - (total samples, time steps, features per time step).\n",
    "* **Labels:**\n",
    "    * Load `labels.pkl` (numerical labels corresponding to composers).\n",
    "    * Load `composer_to_label.pkl` and `label_to_composer.pkl` to map between numerical labels and composer names.\n",
    "\n",
    "You can/should convert these NumPy arrays to PyTorch tensors for your models (e.g., `torch.tensor(data, dtype=torch.float32)` for features, `torch.tensor(labels, dtype=torch.long)` for labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d75b2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21296,
     "status": "ok",
     "timestamp": 1754862111932,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "f6d75b2a",
    "outputId": "b32d3475-61d1-4aca-c7be-1099cbd7f44c"
   },
   "outputs": [],
   "source": [
    "# Define label mappings\n",
    "composer_to_label = {composer: i for i, composer in enumerate(COMPOSERS)}\n",
    "label_to_composer = {i: composer for composer, i in composer_to_label.items()}\n",
    "\n",
    "# Define lists to hold features and labels\n",
    "# These will be used to store the extracted features and corresponding labels for each composer.\n",
    "# They will be converted to NumPy arrays later for model training.\n",
    "features_cnn = []\n",
    "features_lstm = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through each composer\n",
    "for composer in COMPOSERS:\n",
    "    composer_dir = os.path.join(MIDI_DIR)\n",
    "    print(f\"Processing composer: {composer}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            # print(os.path.join(root, file))\n",
    "            # Check if the file is a MIDI file and contains 'bach' in its name.\n",
    "            # There are other composers that need to be processed too.\n",
    "            if (file.endswith('.mid') or file.endswith('.midi')) and composer.lower() in file.lower():\n",
    "                midi_path = os.path.join(root, file)\n",
    "                # print(\"Reading file: \", file)\n",
    "\n",
    "                try:\n",
    "                    segments = extract_segments_from_midi(midi_path, SEGMENT_DURATION_SECONDS, SAMPLES_PER_SECOND)\n",
    "                except Exception as e:\n",
    "                    # print(f\"Skipping {file}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                for segment in segments:\n",
    "                    all_augmented = [segment]\n",
    "\n",
    "                    for step in AUGMENT_TRANSPOSITION_STEPS:\n",
    "                        all_augmented.append(apply_augmentation(segment, 'transpose', step))\n",
    "                    for scale in AUGMENT_TEMPO_SCALES:\n",
    "                        all_augmented.append(apply_augmentation(segment, 'tempo_scale', scale))\n",
    "\n",
    "                    for augmented_segment in all_augmented:\n",
    "                        # CNN Features\n",
    "                        piano_roll = midi_to_piano_roll(augmented_segment, duration=SEGMENT_DURATION_SECONDS,\n",
    "                                                        samples_per_second=SAMPLES_PER_SECOND,\n",
    "                                                        pitch_low=PITCH_LOW, pitch_high=PITCH_HIGH)\n",
    "                        # LSTM Features\n",
    "                        sequential = midi_to_sequential_features(augmented_segment, duration=SEGMENT_DURATION_SECONDS,\n",
    "                                                                 samples_per_second=SAMPLES_PER_SECOND,\n",
    "                                                                 pitch_low=PITCH_LOW, pitch_high=PITCH_HIGH)\n",
    "\n",
    "                        # Append label only if both features were generated\n",
    "                        if piano_roll is not None and sequential is not None:\n",
    "                            features_cnn.append(piano_roll)\n",
    "                            features_lstm.append(sequential)\n",
    "                            labels.append(composer_to_label[composer])\n",
    "\n",
    "print(\"Finished processing all composers.\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "np_features_cnn = np.array(features_cnn, dtype=np.float32)\n",
    "np_features_lstm = np.array(features_lstm, dtype=np.float32)\n",
    "labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "# Save to disk\n",
    "with open(os.path.join(OUTPUT_DIR, 'features_cnn.pkl'), 'wb') as f:\n",
    "    pickle.dump(np_features_cnn, f)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'features_lstm.pkl'), 'wb') as f:\n",
    "    pickle.dump(np_features_lstm, f)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'labels.pkl'), 'wb') as f:\n",
    "    pickle.dump(labels, f)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'composer_to_label.pkl'), 'wb') as f:\n",
    "    pickle.dump(composer_to_label, f)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'label_to_composer.pkl'), 'wb') as f:\n",
    "    pickle.dump(label_to_composer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9201f03e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1754862111975,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "9201f03e",
    "outputId": "635a9bc6-a6a4-4362-ee8f-adbaf8fcb1ef"
   },
   "outputs": [],
   "source": [
    "# Print summary of saved data\n",
    "print(f\"Saved {len(np_features_cnn)} CNN features and {len(np_features_lstm)} LSTM features.\")\n",
    "print(f\"Saved {len(labels)} labeled examples for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8943da3f",
   "metadata": {
    "id": "8943da3f"
   },
   "source": [
    "CNN Input: (batch_size, 1, 88, 500) → channel-first PyTorch format (grayscale piano roll)\n",
    "CNN Output per segment: (batch_size, time_steps=some_N, features_per_step=some_M)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938670e2",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1754862112006,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "938670e2"
   },
   "outputs": [],
   "source": [
    "class ComposerCNN(nn.Module):\n",
    "    def __init__(self, num_pitches, num_time_steps):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        # Use integer division and guard with adaptive pooling if needed\n",
    "        pooled_pitches = max(1, num_pitches // 4)\n",
    "        pooled_steps   = max(1, num_time_steps // 4)\n",
    "        self.fc1 = nn.Linear(64 * pooled_pitches * pooled_steps, 128)\n",
    "        self.fc2 = nn.Linear(128, len(COMPOSERS))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aeb56d",
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1754862112412,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "53aeb56d"
   },
   "outputs": [],
   "source": [
    "model_cnn = ComposerCNN(NUM_PITCHES, int(SEGMENT_DURATION_SECONDS * SAMPLES_PER_SECOND)).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514fe3c",
   "metadata": {
    "id": "9514fe3c"
   },
   "source": [
    "Model Building: Develop a deep learning model using LSTM and CNN architectures to classify the musical scores according to the composer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d17183",
   "metadata": {
    "id": "05d17183"
   },
   "source": [
    "LSTM Input Shape: (batch_size, time_steps, features_per_step) → same as (batch_size, seq_len, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b6969",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1754863229681,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "ba1b6969"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 13         # 12 chroma + 1 note density\n",
    "hidden_size = 128       # Can be tuned\n",
    "num_layers = 2          # Can be tuned\n",
    "num_classes = len(COMPOSERS)    # based on the number of composer labels\n",
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a098d",
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1754863233106,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "fe0a098d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Define the LSTM Model\n",
    "# ------------------------------\n",
    "class ComposerLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(ComposerLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=0.3,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(x)  # output: (batch_size, seq_len, hidden_size)\n",
    "        out = lstm_out[:, -1, :]    # Take last time step\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Initialize model, loss, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lstm_model = ComposerLSTM(input_size, hidden_size, num_layers, num_classes).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c04f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1754863236328,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "b56c04f8",
    "outputId": "41898525-dc7d-478f-aba7-60758b846842"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Load Preprocessed Data\n",
    "# ------------------------------\n",
    "with open(os.path.join(OUTPUT_DIR, 'features_lstm.pkl'), 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open(os.path.join(OUTPUT_DIR, 'labels.pkl'), 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)        # Shape: (N, 500, 13)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)           # Shape: (N,)\n",
    "\n",
    "print(X_tensor.shape)  # Should be (N, 500, 13\n",
    "print(y_tensor.shape)  # Should be (N,)\n",
    "\n",
    "# Ensure the input tensor is 3D: (batch_size, seq_len, input_size)\n",
    "if X_tensor.ndim == 2:\n",
    "    X_tensor = X_tensor.unsqueeze(1)  # Add a dimension for seq_len if missing\n",
    "elif X_tensor.ndim != 3:\n",
    "    raise ValueError(f\"Expected X_tensor to be 2D or 3D, got {X_tensor.ndim}D tensor instead.\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YI6Wd5XIAmLr",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1754863408464,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "YI6Wd5XIAmLr"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b825f",
   "metadata": {
    "id": "e26b825f"
   },
   "source": [
    "Model Training: Train the deep learning model using the pre-processed and feature-extracted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c65da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59989,
     "status": "ok",
     "timestamp": 1754863567575,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "f15c65da",
    "outputId": "438d6cc5-6216-4b7c-aca8-1b5fd7b8abeb"
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Training Loop\n",
    "# ------------------------------\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    lstm_model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(DEVICE), y_val.to(DEVICE)\n",
    "            outputs = lstm_model(X_val)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(lstm_model.state_dict(), \"composer_lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8TNamrKUA3VM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1754863583705,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "8TNamrKUA3VM",
    "outputId": "f9471391-61ec-40cf-8b77-b6895a43890b"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, marker='o', label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, val_accuracies, marker='o', color='orange', label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a6b0eb",
   "metadata": {
    "id": "44a6b0eb"
   },
   "source": [
    "CNN Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de73911d",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1754863619425,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "de73911d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            preds = logits.argmax(1)\n",
    "            y_true.extend(yb.cpu().numpy())\n",
    "            y_pred .extend(preds.cpu().numpy())\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    return acc, prec, rec, f1, confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac96af6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35173,
     "status": "ok",
     "timestamp": 1754862178655,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "aac96af6",
    "outputId": "7c421115-44d5-4e66-fa13-296d3a6c0d7e"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, 'features_cnn.pkl'), 'rb') as f:\n",
    "    Xc = pickle.load(f)  # (N, P, T, 1)\n",
    "Xc_tensor = torch.tensor(Xc, dtype=torch.float32).permute(0, 3, 1, 2)  # -> (N,1,P,T)\n",
    "yc_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "ds_cnn = TensorDataset(Xc_tensor, yc_tensor)\n",
    "train_size = int(0.8 * len(ds_cnn))\n",
    "val_size   = len(ds_cnn) - train_size\n",
    "train_cnn, val_cnn = random_split(ds_cnn, [train_size, val_size])\n",
    "\n",
    "model_cnn = ComposerCNN(NUM_PITCHES, int(SEGMENT_DURATION_SECONDS * SAMPLES_PER_SECOND)).to(DEVICE)\n",
    "opt_cnn = optim.Adam(model_cnn.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_cnn.train()\n",
    "    for xb, yb in DataLoader(train_cnn, batch_size=128, shuffle=True):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt_cnn.zero_grad()\n",
    "        loss = crit(model_cnn(xb), yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_cnn.parameters(), 1.0)\n",
    "        opt_cnn.step()\n",
    "    acc, prec, rec, f1, _ = evaluate_model(model_cnn, DataLoader(val_cnn, batch_size=128), DEVICE)\n",
    "    print(f\"[CNN] Epoch {epoch+1}: acc={acc:.3f} P={prec:.3f} R={rec:.3f} F1={f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96619bbd",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1754863758390,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "96619bbd"
   },
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, out_channels=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, out_channels, kernel_size=3, padding=1)\n",
    "        # pool over pitch only: (2,1) halves pitch, keeps time\n",
    "        self.pool_pitch = nn.MaxPool2d(kernel_size=(2,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, 1, P, T)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool_pitch(x)             # (N, 32, P/2, T)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool_pitch(x)             # (N, C, P/4, T)\n",
    "        # global average over remaining pitch bins -> (N, C, 1, T)\n",
    "        x = x.mean(dim=2, keepdim=True)\n",
    "        x = x.squeeze(2)                   # (N, C, T)\n",
    "        x = x.permute(0, 2, 1)             # (N, T, C)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5ad67",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1754863760264,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "39c5ad67"
   },
   "outputs": [],
   "source": [
    "class FusionLSTM(nn.Module):\n",
    "    def __init__(self, seq_input_size, cnn_feat_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=seq_input_size + cnn_feat_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=False)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, seq_feats, cnn_feats):\n",
    "        # seq_feats: (N, T, 13), cnn_feats: (N, T, C)\n",
    "        x = torch.cat([seq_feats, cnn_feats], dim=-1)  # (N, T, 13+C)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b35c00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 539237,
     "status": "ok",
     "timestamp": 1754862733408,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "c7b35c00",
    "outputId": "bd4aa16c-dc77-4e00-9a83-a188df45b055"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# LSTM features\n",
    "X_seq = torch.tensor(features_lstm, dtype=torch.float32)               # (N, T, 13)\n",
    "\n",
    "# CNN piano-roll features -> (N,1,P,T)\n",
    "X_roll = torch.tensor(features_cnn, dtype=torch.float32).permute(0,3,1,2)\n",
    "y_t = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset for the combined features\n",
    "dataset_full = TensorDataset(X_seq, X_roll, y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa18790",
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1754863784428,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "efa18790"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8*len(dataset_full))\n",
    "train_ds, val_ds = random_split(dataset_full, [train_size, len(dataset_full) - train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accf94b",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1754862733425,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "7accf94b"
   },
   "outputs": [],
   "source": [
    "# For testing, reduce the size of the training set\n",
    "#frac = 0.2  # use 20% for quick tests\n",
    "\n",
    "# Randomly select a subset of the training set\n",
    "#y_t = y_t[train_ds.indices]  # Get the labels for the training set\n",
    "#N = len(y_t)\n",
    "#idx = torch.randperm(N)\n",
    "#sub_idx = idx[:int(N * frac)]\n",
    "\n",
    "#dataset_sub  = Subset(dataset_full, sub_idx)\n",
    "\n",
    "# Split AFTER subsetting (keeps test quick too)\n",
    "#sub_train_size = int(0.8 * len(dataset_sub))\n",
    "#train_ds, val_ds = random_split(dataset_sub, [sub_train_size, len(dataset_sub) - sub_train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70febb24",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1754863816242,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "70febb24"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, pin_memory=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e315d",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1754863820480,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "060e315d"
   },
   "outputs": [],
   "source": [
    "cnn_enc = CNNEncoder(out_channels=64).to(DEVICE)\n",
    "fusion_model   = FusionLSTM(seq_input_size=13, cnn_feat_size=64,\n",
    "                     hidden_size=128, num_layers=2,\n",
    "                     num_classes=len(COMPOSERS), dropout=0.3).to(DEVICE)\n",
    "\n",
    "params = list(cnn_enc.parameters()) + list(fusion_model.parameters())\n",
    "optimizer = optim.Adam(params, lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nD-PIkZ3-nl6",
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1754863822430,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "nD-PIkZ3-nl6"
   },
   "outputs": [],
   "source": [
    "def compute_cls_metrics(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    return acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad55a6b9",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1754863826865,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "ad55a6b9"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_fusion(cnn_enc, fusion_model, loader, device):\n",
    "    cnn_enc.eval(); fusion_model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for seq_b, roll_b, yb in loader:\n",
    "            seq_b, roll_b, yb = seq_b.to(device, non_blocking=True), roll_b.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "            cnn_feats = cnn_enc(roll_b)           # (N,T,C)\n",
    "            logits = fusion_model(seq_b, cnn_feats)\n",
    "            loss = criterion(logits, yb)\n",
    "            running_loss += loss.item()\n",
    "            y_true.extend(yb.cpu().numpy())\n",
    "            y_pred.extend(logits.argmax(1).cpu().numpy())\n",
    "    acc, prec, rec, f1 = compute_cls_metrics(y_true, y_pred)\n",
    "    return running_loss / max(1, len(loader)), acc, prec, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3bf3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123550,
     "status": "ok",
     "timestamp": 1754863953491,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "c3f3bf3e",
    "outputId": "cc9acbda-5d26-43dc-d9be-9b7e97ccb877"
   },
   "outputs": [],
   "source": [
    "logger = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [],\n",
    "          \"val_acc\": [], \"val_prec\": [], \"val_rec\": [], \"val_f1\": []}\n",
    "\n",
    "scaler = torch.amp.GradScaler(DEVICE.type)\n",
    "num_epochs = 20\n",
    "\n",
    "# Training loop for the fusion model\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_enc.train()\n",
    "    fusion_model.train()\n",
    "    running = 0.0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for seq_batch, roll_batch, y_batch in train_loader:\n",
    "        seq_batch = seq_batch.to(DEVICE)          # (N,T,13)\n",
    "        roll_batch = roll_batch.to(DEVICE)        # (N,1,P,T)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(DEVICE.type):\n",
    "          cnn_feats = cnn_enc(roll_batch)           # (N,T,64)\n",
    "          logits = fusion_model(seq_batch, cnn_feats)      # (N,num_classes)\n",
    "          loss = criterion(logits, y_batch)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running += loss.item()\n",
    "\n",
    "    # … your existing evaluate_model(val_loader) adapted to take (seq, roll, y) …\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate_fusion(cnn_enc, fusion_model, val_loader, DEVICE)\n",
    "    print(f\"Train Loss: {running/len(train_loader):.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.4f}, \"\n",
    "          f\"Val Prec: {val_prec:.4f}, \"\n",
    "          f\"Val Rec: {val_rec:.4f}, \"\n",
    "          f\"Val F1: {val_f1:.4f}\")\n",
    "\n",
    "    # Log the results\n",
    "    logger[\"epoch\"].append(epoch+1)\n",
    "    logger[\"train_loss\"].append(running / max(1, len(train_loader)))\n",
    "    logger[\"val_loss\"].append(val_loss)\n",
    "    logger[\"val_acc\"].append(val_acc)\n",
    "    logger[\"val_prec\"].append(val_prec)\n",
    "    logger[\"val_rec\"].append(val_rec)\n",
    "    logger[\"val_f1\"].append(val_f1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | train_loss={logger['train_loss'][-1]:.4f} \"\n",
    "          f\"| val_loss={val_loss:.4f} | acc={val_acc:.3f} | P={val_prec:.3f} | R={val_rec:.3f} | F1={val_f1:.3f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save({\n",
    "    'cnn_encoder_state_dict': cnn_enc.state_dict(),\n",
    "    'fusion_model_state_dict': fusion_model.state_dict(),\n",
    "}, 'composer_fusion_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51863c3",
   "metadata": {
    "id": "c51863c3"
   },
   "source": [
    "Model Evaluation: Evaluate the performance of the deep learning model using accuracy, precision, and recall metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efb64f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1754863966733,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "c1efb64f",
    "outputId": "a0aebaef-ee73-48e6-935e-86d0657ffb88"
   },
   "outputs": [],
   "source": [
    "# Plotting the training and validation metrics\n",
    "epochs = logger[\"epoch\"]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epochs, logger[\"train_loss\"], marker='o', label='Train Loss')\n",
    "plt.plot(epochs, logger[\"val_loss\"],   marker='o', label='Val Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss over Epochs'); plt.legend(); plt.grid(True, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1296e4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1754864009894,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "c1296e4b",
    "outputId": "2481cb8b-2851-4b72-b3c3-7f2ce345511d"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epochs, logger[\"val_acc\"],  marker='o', label='Val Acc')\n",
    "plt.plot(epochs, logger[\"val_prec\"], marker='o', label='Val Precision (macro)')\n",
    "plt.plot(epochs, logger[\"val_rec\"],  marker='o', label='Val Recall (macro)')\n",
    "plt.plot(epochs, logger[\"val_f1\"],   marker='o', label='Val F1 (macro)')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Score'); plt.title('Validation Metrics'); plt.legend(); plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a7f955",
   "metadata": {
    "id": "63a7f955"
   },
   "source": [
    "Model Optimization: Optimize the deep learning model by fine-tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e4eec",
   "metadata": {
    "executionInfo": {
     "elapsed": 666418,
     "status": "aborted",
     "timestamp": 1754862733519,
     "user": {
      "displayName": "James McCarthy",
      "userId": "13323000847911347797"
     },
     "user_tz": 420
    },
    "id": "180e4eec"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1NkC41kyHRnPk6euhnrbjR2uZrC8s1Ny9",
     "timestamp": 1754862049449
    }
   ]
  },
  "kernelspec": {
   "display_name": "aai-511-final-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
