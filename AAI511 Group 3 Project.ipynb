{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Music Composer Identification using Deep Learning\n",
        "\n",
        "The primary objective of this project is to develop a deep learning model that can predict the composer of a given musical score accurately. The project aims to accomplish this objective by using two deep learning techniques: Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN).\n",
        "\n",
        "## Project Team & Responsibilities:\n",
        "\n",
        "* **Dom:** Data Collection, Data Preprocessing (MIDI conversion, segmentation, augmentation), Feature Extraction (Piano Rolls for CNN, Sequential Features for LSTM).\n",
        "* **Santosh:** CNN Model Building, Training, Evaluation, Optimization.\n",
        "* **Jim:** LSTM Model Building, Training, Evaluation, Optimization.\n",
        "\n",
        "## Project Roadmap & Status:\n",
        "\n",
        "Here's a breakdown of our project phases and current status:\n",
        "\n",
        "1.  **Initial Setup & Data Download (COMPLETED by Jim):**\n",
        "    * Basic imports are set up.\n",
        "    * The `blanderbuss/midi-classic-music` dataset has been downloaded from Kaggle.\n",
        "    * *Status:* Ready for data processing.\n",
        "\n",
        "2.  **Data Preprocessing & Feature Extraction (COMPLETED by Dom):**\n",
        "    * **Goal:** Convert raw MIDI files into numerical features (Piano Rolls for CNNs, Sequential Features for LSTMs) and augment dataset.\n",
        "    * **Responsible:** Dom.\n",
        "    * *Current Status:* Completed / Needs implementation of the sections below.\n",
        "\n",
        "3.  **Model Building (NEXT STEP for Team):**\n",
        "    * **Goal:** Design CNN and LSTM model architectures.\n",
        "    * **Responsible:** Santosh (CNN), Jim (LSTM).\n",
        "    * *Dependencies:* Requires processed data from Phase 2.\n",
        "\n",
        "4.  **Model Training & Evaluation (AFTER Model Building):**\n",
        "    * **Goal:** Train the models and evaluate their performance using metrics like accuracy, precision, and recall.\n",
        "    * **Responsible:** Santosh (CNN), Jim (LSTM).\n",
        "    * *Dependencies:* Requires built models from Phase 3.\n",
        "\n",
        "5.  **Model Optimization (Post Training):**\n",
        "    * **Goal:** Fine-tune model hyperparameters to improve performance.\n",
        "    * **Responsible:** Santosh (CNN), Jim (LSTM) & Dom (Feature Engineering).\n",
        "    * *Dependencies:* Requires initial model training."
      ],
      "metadata": {
        "id": "90tetzst6hMs"
      },
      "id": "90tetzst6hMs"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c3d11aba",
      "metadata": {
        "id": "c3d11aba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1348afbb",
      "metadata": {
        "id": "1348afbb"
      },
      "source": [
        "Data Collection\n",
        "The dataset contains the midi files of compositions from well-known classical composers like Bach, Beethoven, Chopin, and Mozart. The dataset has been labeled with the name of the composer for each score. Predictions are performed for only the below composers:\n",
        "\n",
        "1-Bach\n",
        "\n",
        "2-Beethoven\n",
        "\n",
        "3-Chopin\n",
        "\n",
        "4-Mozart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "76171d1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76171d1a",
        "outputId": "642c159a-78aa-480c-f3dc-efff3d140949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.7.9)\n",
            "Path to dataset files: /kaggle/input/midi-classic-music\n"
          ]
        }
      ],
      "source": [
        "%pip install kagglehub\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "de884095",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de884095",
        "outputId": "7ef91ec6-07ec-4b07-b919-f8aa8febe8b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found MIDI file: Liszt Bach Prelude Transcription.mid\n",
            "Found MIDI file: C.P.E.Bach Solfeggieto.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.15.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.6.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.7.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.4.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No...mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.1.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.9.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.14.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.10.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.13.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.12.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.11.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.5.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.8.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.3.mid\n",
            "Found MIDI file: Piano version of Bachs two part inventions No.2.mid\n",
            "Found MIDI file: bachlein.mid\n",
            "Found MIDI file: bach_4.mid\n",
            "Found MIDI file: bach_1.mid\n",
            "Found MIDI file: bach_4p.mid\n",
            "Found MIDI file: bach2.mid\n",
            "Found MIDI file: bach.mid\n",
            "Found MIDI file: bach197c.mid\n",
            "Found MIDI file: bach_10p.mid\n",
            "Found MIDI file: bachaire.mid\n",
            "Found MIDI file: bachpfam.mid\n",
            "Found MIDI file: bachmen7.mid\n",
            "Found MIDI file: bach_13p.mid\n",
            "Found MIDI file: bach12a.mid\n",
            "Found MIDI file: bachmen6.mid\n",
            "Found MIDI file: bach0.mid\n",
            "Found MIDI file: bach_14.mid\n",
            "Found MIDI file: bachinv.mid\n",
            "Found MIDI file: bachg.mid\n",
            "Found MIDI file: bachto.mid\n",
            "Found MIDI file: bachpart.mid\n",
            "Found MIDI file: Six variations on Bach No.1.mid\n",
            "Found MIDI file: Six variations on Bach No.3.mid\n",
            "Found MIDI file: Fantasia Nach J. S. Bach.mid\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# List all files in the dataset path\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        #print(os.path.join(root, file))\n",
        "        # Check if the file is a MIDI file and contains 'bach' in its name.\n",
        "        # There are other composers that need to be processed too.\n",
        "        if (file.endswith('.mid') or file.endswith('.midi')) and 'bach' in file.lower():\n",
        "            print(f\"Found MIDI file: {file}\")\n",
        "            # Add file to Bach dataset processing logic here\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a8eac3",
      "metadata": {
        "id": "b1a8eac3"
      },
      "source": [
        "Convert MIDI file to something useful for LSTM and CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fb11fd45",
      "metadata": {
        "id": "fb11fd45"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import glob\n",
        "import music21\n",
        "import pretty_midi\n",
        "import numpy as np # Already imported, but good to have here for clarity for my feature engineering\n",
        "import pickle\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I will place these here so they run after Kaggle download, as I encountered conflicts with the initial setup when adding above.\n",
        "!pip install music21\n",
        "!pip install pretty_midi\n",
        "!pip install --upgrade numpy # Ensure I have a recent numpy version"
      ],
      "metadata": {
        "id": "oBmREYtE1Nwc",
        "outputId": "2ba9602c-cd7e-4f36-d11b-c41f363ab958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oBmREYtE1Nwc",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: music21 in /usr/local/lib/python3.11/dist-packages (9.3.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from music21) (5.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from music21) (1.5.1)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.11/dist-packages (from music21) (4.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from music21) (3.10.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from music21) (10.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from music21) (2.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from music21) (2.32.3)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.11/dist-packages (from music21) (24.11.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->music21) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->music21) (2025.7.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->music21) (1.17.0)\n",
            "Requirement already satisfied: pretty_midi in /usr/local/lib/python3.11/dist-packages (0.2.10)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.3.1)\n",
            "Requirement already satisfied: mido>=1.1.16 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca6f3a3",
      "metadata": {
        "id": "fca6f3a3"
      },
      "source": [
        "Data Pre-processing: Convert the musical scores into a format suitable for deep learning models. This involves converting the musical scores into MIDI files and applying data augmentation techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing and Feature Extraction\n",
        "KAGGLE_DOWNLOAD_PATH = \"/root/.cache/kagglehub/datasets/blanderbuss/midi-classic-music/versions/1\"\n",
        "MIDI_DIR = KAGGLE_DOWNLOAD_PATH\n",
        "\n",
        "OUTPUT_DIR = \"/content/processed_data\"\n",
        "SEGMENT_DURATION_SECONDS = 5\n",
        "SAMPLES_PER_SECOND = 100\n",
        "\n",
        "PITCH_LOW = 21\n",
        "PITCH_HIGH = 108\n",
        "NUM_PITCHES = PITCH_HIGH - PITCH_LOW + 1\n",
        "\n",
        "AUGMENT_TRANSPOSITION_STEPS = [-3, -2, -1, 1, 2, 3]\n",
        "AUGMENT_TEMPO_SCALES = [0.9, 1.1]\n",
        "\n",
        "# Defines composers\n",
        "COMPOSERS = [\"Bach\", \"Beethoven\", \"Chopin\", \"Mozart\"]\n",
        "\n",
        "# Creates output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"MIDI data will be processed from: {MIDI_DIR}\")\n",
        "print(f\"Processed data will be saved to: {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "85boMSuo1Uhi",
        "outputId": "621d663d-de86-42e8-e4d6-f9f47dfb8515",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "85boMSuo1Uhi",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MIDI data will be processed from: /root/.cache/kagglehub/datasets/blanderbuss/midi-classic-music/versions/1\n",
            "Processed data will be saved to: /content/processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Feature Extraction : Extract features from the MIDI files, such as notes, chords, and tempo, using music analysis tools.\n",
        "\n",
        "Here, the prepared MIDI segments are converted into numerical representations. I've generated different formats for the CNN and LSTM models to leverage the strengths of each.\n",
        "\n",
        "* **For CNNs: The Piano Roll**\n",
        "    * **Purpose:** CNNs excel at recognizing visual patterns. A piano roll converts music into a 2D image (pitch vs. time), allowing the CNN to \"see\" and learn characteristic melodic shapes, harmonic voicings, and rhythmic patterns that define a composer's style.\n",
        "    * **Details:** The piano roll captures note activity (velocity) across a defined pitch range (MIDI 21-108) over time, sampled at 100 samples per second. All outputs are normalized to [0,1] and padded/truncated to a consistent shape.\n",
        "* **For LSTMs: Sequential Features (Chroma & Note Density)**\n",
        "    * **Purpose:** LSTMs are great tools for understanding temporal sequences. These features describe the harmonic content and musical activity at each point in time, allowing the LSTM to learn how a composer's musical ideas evolve.\n",
        "    * **Details:** Each time step in the sequence contains a 12-element Pitch Class Profile (Chroma) representing harmonic presence (e.g., C, C#, D) and a single value for overall note density/volume. These are also sampled at 100 samples per second and normalized."
      ],
      "metadata": {
        "id": "R-F6z-ax4dPw"
      },
      "id": "R-F6z-ax4dPw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction - midi_to_sequential_features (for LSTMs)\n",
        "# This function extracts time-series features like Pitch Class Profiles and note density from a MIDI segment for LSTMs\n",
        "\n",
        "def midi_to_sequential_features(midi_data_segment: pretty_midi.PrettyMIDI, duration: float,\n",
        "                                samples_per_second: int, pitch_low: int, pitch_high: int) -> np.ndarray:\n",
        "    if not midi_data_segment.instruments:\n",
        "        return None\n",
        "\n",
        "    num_target_time_steps = int(duration * samples_per_second)\n",
        "    num_features_per_timestep = 12 + 1 # Chroma + Note Density\n",
        "    sequential_features = np.zeros((num_target_time_steps, num_features_per_timestep), dtype=np.float32)\n",
        "\n",
        "    chroma_features = midi_data_segment.get_chroma(fs=samples_per_second)\n",
        "    chroma_features = chroma_features.T\n",
        "\n",
        "    note_density = np.zeros(num_target_time_steps, dtype=np.float32)\n",
        "    for instrument in midi_data_segment.instruments:\n",
        "        for note in instrument.notes:\n",
        "            start_idx = int(note.start * samples_per_second)\n",
        "            end_idx = int(note.end * samples_per_second)\n",
        "            start_idx = max(0, min(start_idx, num_target_time_steps - 1))\n",
        "            end_idx = max(0, min(end_idx, num_target_time_steps - 1))\n",
        "            if end_idx >= start_idx:\n",
        "                note_density[start_idx:end_idx] += note.velocity\n",
        "\n",
        "    max_density = np.max(note_density)\n",
        "    if max_density > 0:\n",
        "        note_density /= max_density\n",
        "\n",
        "    sequential_features[:, :12] = chroma_features[:num_target_time_steps, :]\n",
        "    sequential_features[:, 12] = note_density[:num_target_time_steps]\n",
        "\n",
        "    current_time_steps = sequential_features.shape[0]\n",
        "    if current_time_steps < num_target_time_steps:\n",
        "        padding_needed = num_target_time_steps - current_time_steps\n",
        "        sequential_features = np.pad(sequential_features, ((0, padding_needed), (0, 0)), mode='constant')\n",
        "    elif current_time_steps > num_target_time_steps:\n",
        "        sequential_features = sequential_features[:num_target_time_steps, :]\n",
        "    return sequential_features"
      ],
      "metadata": {
        "id": "ZB8qbxWy2Xox"
      },
      "id": "ZB8qbxWy2Xox",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction - midi_to_piano_roll (for CNNs)\n",
        "# This function converts a MIDI segment into a 2D image-like \"piano roll\" for CNNs.\n",
        "\n",
        "def midi_to_piano_roll(midi_data_segment: pretty_midi.PrettyMIDI, duration: float,\n",
        "                        samples_per_second: int, pitch_low: int, pitch_high: int) -> np.ndarray:\n",
        "    if not midi_data_segment.instruments:\n",
        "        return None\n",
        "\n",
        "    piano_roll = midi_data_segment.get_piano_roll(fs=samples_per_second,\n",
        "                                                low=pitch_low, high=pitch_high)\n",
        "    piano_roll = piano_roll / 127.0\n",
        "\n",
        "    num_target_time_steps = int(duration * samples_per_second)\n",
        "    current_time_steps = piano_roll.shape[1]\n",
        "\n",
        "    if current_time_steps < num_target_time_steps:\n",
        "        padding_needed = num_target_time_steps - current_time_steps\n",
        "        piano_roll = np.pad(piano_roll, ((0, 0), (0, padding_needed)), mode='constant')\n",
        "    elif current_time_steps > num_target_time_steps:\n",
        "        piano_roll = piano_roll[:, :num_target_time_steps]\n",
        "\n",
        "    return piano_roll.reshape(NUM_PITCHES, num_target_time_steps, 1)"
      ],
      "metadata": {
        "id": "ykozm5Z92Xs1"
      },
      "id": "ykozm5Z92Xs1",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Function - create_pretty_midi_segment\n",
        "# This function extracts a specific time segment from a larger MIDI file.\n",
        "\n",
        "def create_pretty_midi_segment(full_midi_data: pretty_midi.PrettyMIDI, start_time: float, end_time: float) -> pretty_midi.PrettyMIDI:\n",
        "    segment_pm = pretty_midi.PrettyMIDI()\n",
        "    for instrument in full_midi_data.instruments:\n",
        "        new_instrument = pretty_midi.Instrument(program=instrument.program, is_drum=instrument.is_drum, name=instrument.name)\n",
        "        for note in instrument.notes:\n",
        "            if note.end > start_time and note.start < end_time:\n",
        "                new_note = pretty_midi.Note(\n",
        "                    velocity=note.velocity,\n",
        "                    pitch=note.pitch,\n",
        "                    start=max(0.0, note.start - start_time),\n",
        "                    end=min(end_time - start_time, note.end - start_time)\n",
        "                )\n",
        "                if new_note.end > new_note.start:\n",
        "                    new_instrument.notes.append(new_note)\n",
        "        if new_instrument.notes:\n",
        "            segment_pm.instruments.append(new_instrument)\n",
        "    return segment_pm"
      ],
      "metadata": {
        "id": "DhDQAVpr17wT"
      },
      "id": "DhDQAVpr17wT",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility Function - apply_augmentation\n",
        "# This function modifies a MIDI segment by transposing its pitch or scaling its tempo.\n",
        "\n",
        "def apply_augmentation(midi_data_segment: pretty_midi.PrettyMIDI, augmentation_type: str, value) -> pretty_midi.PrettyMIDI:\n",
        "    augmented_midi = pretty_midi.PrettyMIDI()\n",
        "    for instrument in midi_data_segment.instruments:\n",
        "        new_instrument = pretty_midi.Instrument(program=instrument.program, is_drum=instrument.is_drum, name=instrument.name)\n",
        "        for note in instrument.notes:\n",
        "            new_note = pretty_midi.Note(note.velocity, note.pitch, note.start, note.end)\n",
        "            new_instrument.notes.append(new_note)\n",
        "        augmented_midi.instruments.append(new_instrument)\n",
        "\n",
        "    if augmentation_type == 'transpose':\n",
        "        for instrument in augmented_midi.instruments:\n",
        "            for note in instrument.notes:\n",
        "                note.pitch = int(max(0, min(127, note.pitch + value)))\n",
        "    elif augmentation_type == 'tempo_scale':\n",
        "        for instrument in augmented_midi.instruments:\n",
        "            for note in instrument.notes:\n",
        "                note.start *= value\n",
        "                note.end *= value\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown augmentation type: {augmentation_type}\")\n",
        "    return augmented_midi"
      ],
      "metadata": {
        "id": "dmwdendo2XvZ"
      },
      "id": "dmwdendo2XvZ",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Processing Loop & Output Conclusion\n",
        "\n",
        "This section orchestrates the loading of MIDI files, segmenting them, applying all augmentations, extracting features, and finally saving the processed data.\n",
        "\n",
        "* **Process:** Iterates through each composer's MIDI files, segments them, applies both transposition and tempo scaling for each segment, and then generates both CNN and LSTM features.\n",
        "* **Output Data:** The processed features and corresponding labels are saved as `.pkl` files in the `/content/processed_data/` directory.\n",
        "\n",
        "---\n",
        "\n",
        "#### **The data is ready for model training!**\n",
        "\n",
        "* **For CNN Model (Santosh):**\n",
        "    * Load `features_cnn.pkl`.\n",
        "    * Expected input shape: `(num_segments, 88, 500, 1)` - (total samples, pitches, time steps, channels).\n",
        "* **For LSTM Model (Jim):**\n",
        "    * Load `features_lstm.pkl`.\n",
        "    * Expected input shape: `(num_segments, 500, 13)` - (total samples, time steps, features per time step).\n",
        "* **Labels:**\n",
        "    * Load `labels.pkl` (numerical labels corresponding to composers).\n",
        "    * Load `composer_to_label.pkl` and `label_to_composer.pkl` to map between numerical labels and composer names.\n",
        "\n",
        "YOu can/should convert these NumPy arrays to PyTorch tensors for your models (e.g., `torch.tensor(data, dtype=torch.float32)` for features, `torch.tensor(labels, dtype=torch.long)` for labels).\n"
      ],
      "metadata": {
        "id": "J1_k_NN85H2c"
      },
      "id": "J1_k_NN85H2c"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYbkhNg42XXr"
      },
      "id": "DYbkhNg42XXr",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "938670e2",
      "metadata": {
        "id": "938670e2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9514fe3c",
      "metadata": {
        "id": "9514fe3c"
      },
      "source": [
        "Model Building: Develop a deep learning model using LSTM and CNN architectures to classify the musical scores according to the composer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ba1b6969",
      "metadata": {
        "id": "ba1b6969"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e26b825f",
      "metadata": {
        "id": "e26b825f"
      },
      "source": [
        "Model Training: Train the deep learning model using the pre-processed and feature-extracted data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f15c65da",
      "metadata": {
        "id": "f15c65da"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c51863c3",
      "metadata": {
        "id": "c51863c3"
      },
      "source": [
        "Model Evaluation: Evaluate the performance of the deep learning model using accuracy, precision, and recall metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ad55a6b9",
      "metadata": {
        "id": "ad55a6b9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "63a7f955",
      "metadata": {
        "id": "63a7f955"
      },
      "source": [
        "Model Optimization: Optimize the deep learning model by fine-tuning hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "180e4eec",
      "metadata": {
        "id": "180e4eec"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}