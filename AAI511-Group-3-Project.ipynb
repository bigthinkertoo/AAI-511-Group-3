{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "90tetzst6hMs",
      "metadata": {
        "id": "90tetzst6hMs"
      },
      "source": [
        "# Music Composer Identification using Deep Learning\n",
        "\n",
        "The primary objective of this project is to develop a deep learning model that can predict the composer of a given musical score accurately. The project aims to accomplish this objective by using two deep learning techniques: Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN).\n",
        "\n",
        "## Project Team & Responsibilities:\n",
        "\n",
        "* **Dom:** Data Collection, Data Preprocessing (MIDI conversion, segmentation, augmentation), Feature Extraction (Piano Rolls for CNN, Sequential Features for LSTM).\n",
        "* **Santosh:** CNN Model Building, Training, Evaluation, Optimization.\n",
        "* **Jim:** LSTM Model Building, Training, Evaluation, Optimization.\n",
        "\n",
        "## Project Roadmap & Status:\n",
        "\n",
        "Here's a breakdown of our project phases and current status:\n",
        "\n",
        "1.  **Initial Setup & Data Download (COMPLETED by Jim):**\n",
        "    * Basic imports are set up.\n",
        "    * The `blanderbuss/midi-classic-music` dataset has been downloaded from Kaggle.\n",
        "    * *Status:* Ready for data processing.\n",
        "\n",
        "2.  **Data Preprocessing & Feature Extraction (COMPLETED by Dom):**\n",
        "    * **Goal:** Convert raw MIDI files into numerical features (Piano Rolls for CNNs, Sequential Features for LSTMs) and augment dataset.\n",
        "    * **Responsible:** Dom.\n",
        "    * *Current Status:* Completed / Needs implementation of the sections below.\n",
        "\n",
        "3.  **Model Building (NEXT STEP for Team):**\n",
        "    * **Goal:** Design CNN and LSTM model architectures.\n",
        "    * **Responsible:** Santosh (CNN), Jim (LSTM).\n",
        "    * *Dependencies:* Requires processed data from Phase 2.\n",
        "\n",
        "4.  **Model Training & Evaluation (AFTER Model Building):**\n",
        "    * **Goal:** Train the models and evaluate their performance using metrics like accuracy, precision, and recall.\n",
        "    * **Responsible:** Santosh (CNN), Jim (LSTM).\n",
        "    * *Dependencies:* Requires built models from Phase 3.\n",
        "\n",
        "5.  **Model Optimization (Post Training):**\n",
        "    * **Goal:** Fine-tune model hyperparameters to improve performance.\n",
        "    * **Responsible:** Santosh (CNN), Jim (LSTM) & Dom (Feature Engineering).\n",
        "    * *Dependencies:* Requires initial model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d11aba",
      "metadata": {
        "id": "c3d11aba"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1348afbb",
      "metadata": {
        "id": "1348afbb"
      },
      "source": [
        "Data Collection\n",
        "The dataset contains the midi files of compositions from well-known classical composers like Bach, Beethoven, Chopin, and Mozart. The dataset has been labeled with the name of the composer for each score. Predictions are performed for only the below composers:\n",
        "\n",
        "1-Bach\n",
        "\n",
        "2-Beethoven\n",
        "\n",
        "3-Chopin\n",
        "\n",
        "4-Mozart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76171d1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76171d1a",
        "outputId": "642c159a-78aa-480c-f3dc-efff3d140949"
      },
      "outputs": [],
      "source": [
        "#%pip install kagglehub\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"blanderbuss/midi-classic-music\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de884095",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de884095",
        "outputId": "7ef91ec6-07ec-4b07-b919-f8aa8febe8b2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# List all files in the dataset path\n",
        "for root, dirs, files in os.walk(path):\n",
        "    for file in files:\n",
        "        #print(os.path.join(root, file))\n",
        "        # Check if the file is a MIDI file and contains 'bach' in its name.\n",
        "        # There are other composers that need to be processed too.\n",
        "        if (file.endswith('.mid') or file.endswith('.midi')) and 'bach' in file.lower():\n",
        "            print(f\"Found MIDI file: {file}\")\n",
        "            # Add file to Bach dataset processing logic here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1a8eac3",
      "metadata": {
        "id": "b1a8eac3"
      },
      "source": [
        "Convert MIDI file to something useful for LSTM and CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oBmREYtE1Nwc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBmREYtE1Nwc",
        "outputId": "2ba9602c-cd7e-4f36-d11b-c41f363ab958"
      },
      "outputs": [],
      "source": [
        "# I will place these here so they run after Kaggle download, as I encountered conflicts with the initial setup when adding above.\n",
        "#!pip install music21\n",
        "#!pip install pretty_midi\n",
        "#!pip install --upgrade numpy # Ensure I have a recent numpy version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb11fd45",
      "metadata": {
        "id": "fb11fd45"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import glob\n",
        "import music21\n",
        "import pretty_midi\n",
        "import numpy as np # Already imported, but good to have here for clarity for my feature engineering\n",
        "import pickle\n",
        "import collections\n",
        "import os\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca6f3a3",
      "metadata": {
        "id": "fca6f3a3"
      },
      "source": [
        "Data Pre-processing: Convert the musical scores into a format suitable for deep learning models. This involves converting the musical scores into MIDI files and applying data augmentation techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85boMSuo1Uhi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85boMSuo1Uhi",
        "outputId": "621d663d-de86-42e8-e4d6-f9f47dfb8515"
      },
      "outputs": [],
      "source": [
        "# Data Preprocessing and Feature Extraction\n",
        "HOME_DIR = Path.home()\n",
        "KAGGLE_DOWNLOAD_PATH = HOME_DIR / \".cache\" / \"kagglehub\" / \"datasets\" / \"blanderbuss\" / \"midi-classic-music\" / \"versions\" / \"1\"\n",
        "MIDI_DIR = str(KAGGLE_DOWNLOAD_PATH)\n",
        "\n",
        "OUTPUT_DIR = \"./content/processed_data/\"\n",
        "SEGMENT_DURATION_SECONDS = 5\n",
        "SAMPLES_PER_SECOND = 100\n",
        "\n",
        "PITCH_LOW = 21\n",
        "PITCH_HIGH = 108\n",
        "NUM_PITCHES = PITCH_HIGH - PITCH_LOW + 1\n",
        "\n",
        "AUGMENT_TRANSPOSITION_STEPS = [-3, -2, -1, 1, 2, 3]\n",
        "AUGMENT_TEMPO_SCALES = [0.9, 1.1]\n",
        "\n",
        "# Defines composers\n",
        "COMPOSERS = [\"Bach\", \"Beethoven\", \"Chopin\", \"Mozart\"]\n",
        "\n",
        "# Creates output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"MIDI data will be processed from: {MIDI_DIR}\")\n",
        "print(f\"Processed data will be saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R-F6z-ax4dPw",
      "metadata": {
        "id": "R-F6z-ax4dPw"
      },
      "source": [
        "###Feature Extraction : Extracts features from the MIDI files, such as notes, chords, and tempo, using music analysis tools.\n",
        "\n",
        "Here, the preprocessed MIDI segments are converted into numerical representations. I've generated different formats for the CNN and LSTM models to leverage the strengths of each.\n",
        "\n",
        "* **For CNNs: The Piano Roll**\n",
        "    * **Purpose:** CNNs excel at recognizing visual patterns. A piano roll converts music into a 2D image (pitch vs. time), allowing the CNN to \"see\" and learn characteristic melodic shapes, harmonic voicings, and rhythmic patterns that define a composer's style.\n",
        "    * **Details:** The piano roll captures note activity (velocity) across a defined pitch range (MIDI 21-108) over time, sampled at 100 samples per second. All outputs are normalized to [0,1] and padded/truncated to a consistent shape.\n",
        "* **For LSTMs: Sequential Features (Chroma & Note Density)**\n",
        "    * **Purpose:** LSTMs are great tools for understanding temporal sequences. These features describe the harmonic content and musical activity at each point in time, allowing the LSTM to learn how a composer's musical ideas evolve.\n",
        "    * **Details:** Each time step in the sequence contains a 12-element Pitch Class Profile (Chroma) representing harmonic presence (e.g., C, C#, D) and a single value for overall note density/volume. These are also sampled at 100 samples per second and normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZB8qbxWy2Xox",
      "metadata": {
        "id": "ZB8qbxWy2Xox"
      },
      "outputs": [],
      "source": [
        "# Feature Extraction - midi_to_sequential_features (for LSTMs)\n",
        "# This function extracts time-series features like Pitch Class Profiles and note density from a MIDI segment for LSTMs\n",
        "\n",
        "def midi_to_sequential_features(midi_data_segment: pretty_midi.PrettyMIDI, duration: float,\n",
        "                                samples_per_second: int, pitch_low: int, pitch_high: int) -> np.ndarray:\n",
        "    if not midi_data_segment.instruments:\n",
        "        return None\n",
        "\n",
        "    num_target_time_steps = int(duration * samples_per_second)\n",
        "    num_features_per_timestep = 12 + 1 # Chroma + Note Density\n",
        "    sequential_features = np.zeros((num_target_time_steps, num_features_per_timestep), dtype=np.float32)\n",
        "\n",
        "    chroma_features = midi_data_segment.get_chroma(fs=samples_per_second).T\n",
        "    print(\"Original chroma shape:\", chroma_features.shape)  # should be (12, T)\n",
        "    if chroma_features.shape[0] < num_target_time_steps:\n",
        "        padding_needed = num_target_time_steps - chroma_features.shape[0]\n",
        "        chroma_features = np.pad(chroma_features, ((0, padding_needed), (0, 0)), mode='constant')\n",
        "    elif chroma_features.shape[0] > num_target_time_steps:\n",
        "        chroma_features = chroma_features[:num_target_time_steps, :]\n",
        "    \n",
        "    note_density = np.zeros(num_target_time_steps, dtype=np.float32)\n",
        "    for instrument in midi_data_segment.instruments:\n",
        "        for note in instrument.notes:\n",
        "            start_idx = int(note.start * samples_per_second)\n",
        "            end_idx = int(note.end * samples_per_second)\n",
        "            start_idx = max(0, min(start_idx, num_target_time_steps - 1))\n",
        "            end_idx = max(0, min(end_idx, num_target_time_steps - 1))\n",
        "            if end_idx >= start_idx:\n",
        "                note_density[start_idx:end_idx] += note.velocity\n",
        "\n",
        "    max_density = np.max(note_density)\n",
        "    if max_density > 0:\n",
        "        note_density /= max_density\n",
        "\n",
        "    sequential_features[:, :12] = chroma_features\n",
        "    sequential_features[:, 12] = note_density\n",
        "\n",
        "    return sequential_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ykozm5Z92Xs1",
      "metadata": {
        "id": "ykozm5Z92Xs1"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "# Feature Extraction - midi_to_piano_roll (for CNNs)\n",
        "# This function converts a MIDI segment into a 2D image-like \"piano roll\" for CNNs.\n",
        "def is_piano(instrument: pretty_midi.Instrument) -> bool:\n",
        "    # Check program number (0-7 are all piano-related in General MIDI)\n",
        "    return not instrument.is_drum and 0 <= instrument.program <= 7\n",
        "\n",
        "def midi_to_piano_roll(midi_data_segment: pretty_midi.PrettyMIDI, duration: float,\n",
        "                        samples_per_second: int, pitch_low: int, pitch_high: int) -> Optional[np.ndarray]:\n",
        "    if not midi_data_segment.instruments:\n",
        "        return None\n",
        "    piano = None # Default instrument of acoustic piano, will be updated if a piano instrument is found\n",
        "    for instrument in midi_data_segment.instruments:\n",
        "        if is_piano(instrument):\n",
        "            piano = instrument\n",
        "    if (piano is None):\n",
        "        print(\"No piano instrument found in MIDI segment.\")\n",
        "        return None\n",
        "    \n",
        "    # Fix: Use 'times' parameter and slice the piano roll to get the desired pitch range\n",
        "    piano_roll = piano.get_piano_roll(fs=samples_per_second)\n",
        "    \n",
        "    # Slice to get the desired pitch range (pitch_low to pitch_high)\n",
        "    piano_roll = piano_roll[pitch_low:pitch_high+1, :]\n",
        "    piano_roll = piano_roll / 127.0\n",
        "\n",
        "    num_target_time_steps = int(duration * samples_per_second)\n",
        "    num_pitches = pitch_high - pitch_low + 1  # Should be 88\n",
        "    current_time_steps = piano_roll.shape[1]\n",
        "\n",
        "    if current_time_steps < num_target_time_steps:\n",
        "        padding = np.zeros((num_pitches, num_target_time_steps - current_time_steps), dtype=np.float32)\n",
        "        piano_roll = np.hstack([piano_roll, padding])\n",
        "    elif current_time_steps > num_target_time_steps:\n",
        "        piano_roll = piano_roll[:, :num_target_time_steps]\n",
        "\n",
        "    return piano_roll.reshape(num_pitches, num_target_time_steps, 1)\n",
        "\n",
        "# def midi_to_piano_roll(midi_data_segment: pretty_midi.PrettyMIDI, duration: float,\n",
        "#                         samples_per_second: int, pitch_low: int, pitch_high: int) -> Optional[np.ndarray]:\n",
        "#     if not midi_data_segment.instruments:\n",
        "#         return None\n",
        "#     piano = None # Default instrument of acoustic piano, will be updated if a piano instrument is found\n",
        "#     for instrument in midi_data_segment.instruments:\n",
        "#         if is_piano(instrument):\n",
        "#             piano = instrument\n",
        "#     if (piano is None):\n",
        "#         print(\"No piano instrument found in MIDI segment.\")\n",
        "#         return None\n",
        "#     piano_roll = piano.get_piano_roll(fs=samples_per_second, low=pitch_low, high=pitch_high)\n",
        "#     piano_roll = piano_roll / 127.0\n",
        "\n",
        "#     num_target_time_steps = int(duration * samples_per_second)\n",
        "#     num_pitches = pitch_high - pitch_low  # Should be 88\n",
        "#     current_time_steps = piano_roll.shape[1]\n",
        "\n",
        "#     if current_time_steps < num_target_time_steps:\n",
        "#         padding = np.zeros((num_pitches, num_target_time_steps - current_time_steps), dtype=np.float32)\n",
        "#         piano_roll = np.hstack([piano_roll, padding])\n",
        "#     elif current_time_steps > num_target_time_steps:\n",
        "#         piano_roll = piano_roll[:, :num_target_time_steps]\n",
        "\n",
        "#     #if current_time_steps < num_target_time_steps:\n",
        "#     #    padding_needed = num_target_time_steps - current_time_steps\n",
        "#     #    piano_roll = np.pad(piano_roll, ((0, 0), (0, padding_needed)), mode='constant')\n",
        "#     #elif current_time_steps > num_target_time_steps:\n",
        "#     #    piano_roll = piano_roll[:, :num_target_time_steps]\n",
        "\n",
        "#     return piano_roll.reshape(num_pitches, num_target_time_steps, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DhDQAVpr17wT",
      "metadata": {
        "id": "DhDQAVpr17wT"
      },
      "outputs": [],
      "source": [
        "# Utility Function - create_pretty_midi_segment\n",
        "# This function extracts a specific time segment from a larger MIDI file.\n",
        "\n",
        "def create_pretty_midi_segment(full_midi_data: pretty_midi.PrettyMIDI, start_time: float, end_time: float) -> pretty_midi.PrettyMIDI:\n",
        "    segment_pm = pretty_midi.PrettyMIDI()\n",
        "    for instrument in full_midi_data.instruments:\n",
        "        new_instrument = pretty_midi.Instrument(program=instrument.program, is_drum=instrument.is_drum, name=instrument.name)\n",
        "        for note in instrument.notes:\n",
        "            if note.end > start_time and note.start < end_time:\n",
        "                new_note = pretty_midi.Note(\n",
        "                    velocity=note.velocity,\n",
        "                    pitch=note.pitch,\n",
        "                    start=max(0.0, note.start - start_time),\n",
        "                    end=min(end_time - start_time, note.end - start_time)\n",
        "                )\n",
        "                if new_note.end > new_note.start:\n",
        "                    new_instrument.notes.append(new_note)\n",
        "        if new_instrument.notes:\n",
        "            segment_pm.instruments.append(new_instrument)\n",
        "    return segment_pm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dmwdendo2XvZ",
      "metadata": {
        "id": "dmwdendo2XvZ"
      },
      "outputs": [],
      "source": [
        "# Utility Function - apply_augmentation\n",
        "# This function modifies a MIDI segment by transposing its pitch or scaling its tempo.\n",
        "\n",
        "def apply_augmentation(midi_data_segment: pretty_midi.PrettyMIDI, augmentation_type: str, value) -> pretty_midi.PrettyMIDI:\n",
        "    augmented_midi = pretty_midi.PrettyMIDI()\n",
        "    for instrument in midi_data_segment.instruments:\n",
        "        new_instrument = pretty_midi.Instrument(program=instrument.program, is_drum=instrument.is_drum, name=instrument.name)\n",
        "        for note in instrument.notes:\n",
        "            new_note = pretty_midi.Note(note.velocity, note.pitch, note.start, note.end)\n",
        "            new_instrument.notes.append(new_note)\n",
        "        augmented_midi.instruments.append(new_instrument)\n",
        "\n",
        "    if augmentation_type == 'transpose':\n",
        "        for instrument in augmented_midi.instruments:\n",
        "            for note in instrument.notes:\n",
        "                note.pitch = int(max(0, min(127, note.pitch + value)))\n",
        "    elif augmentation_type == 'tempo_scale':\n",
        "        for instrument in augmented_midi.instruments:\n",
        "            for note in instrument.notes:\n",
        "                note.start *= value\n",
        "                note.end *= value\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown augmentation type: {augmentation_type}\")\n",
        "    return augmented_midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4783bed0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_segments_from_midi(midi_path, segment_duration=5.0, samples_per_second=100):\n",
        "    try:\n",
        "        full_midi = pretty_midi.PrettyMIDI(midi_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {midi_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "    total_duration = full_midi.get_end_time()\n",
        "    segments = []\n",
        "\n",
        "    for start_time in np.arange(0, total_duration, segment_duration):\n",
        "        end_time = min(start_time + segment_duration, total_duration)\n",
        "\n",
        "        segment = pretty_midi.PrettyMIDI()\n",
        "        for instrument in full_midi.instruments:\n",
        "            new_instrument = pretty_midi.Instrument(program=instrument.program, is_drum=instrument.is_drum)\n",
        "            for note in instrument.notes:\n",
        "                if start_time <= note.start < end_time:\n",
        "                    new_note = pretty_midi.Note(\n",
        "                        velocity=note.velocity,\n",
        "                        pitch=note.pitch,\n",
        "                        start=note.start - start_time,\n",
        "                        end=min(note.end, end_time) - start_time\n",
        "                    )\n",
        "                    new_instrument.notes.append(new_note)\n",
        "            if new_instrument.notes:\n",
        "                segment.instruments.append(new_instrument)\n",
        "\n",
        "        # Only append segments with valid instruments\n",
        "        if segment.instruments:\n",
        "            segments.append(segment)\n",
        "\n",
        "    return segments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J1_k_NN85H2c",
      "metadata": {
        "id": "J1_k_NN85H2c"
      },
      "source": [
        "### Data Processing Loop & Output Conclusion\n",
        "\n",
        "This section orchestrates the loading of MIDI files, segmenting them, applying all augmentations, extracting features, and finally saving the processed data.\n",
        "\n",
        "* **Process:** Iterates through each composer's MIDI files, segments them, applies both transposition and tempo scaling for each segment, and then generates both CNN and LSTM features.\n",
        "* **Output Data:** The processed features and corresponding labels are saved as `.pkl` files in the `/content/processed_data/` directory.\n",
        "\n",
        "---\n",
        "\n",
        "#### **The data is ready for model training!**\n",
        "\n",
        "* **For CNN Model (Santosh):**\n",
        "    * Load `features_cnn.pkl`.\n",
        "    * Expected input shape: `(num_segments, 88, 500, 1)` - (total samples, pitches, time steps, channels).\n",
        "* **For LSTM Model (Jim):**\n",
        "    * Load `features_lstm.pkl`.\n",
        "    * Expected input shape: `(num_segments, 500, 13)` - (total samples, time steps, features per time step).\n",
        "* **Labels:**\n",
        "    * Load `labels.pkl` (numerical labels corresponding to composers).\n",
        "    * Load `composer_to_label.pkl` and `label_to_composer.pkl` to map between numerical labels and composer names.\n",
        "\n",
        "You can/should convert these NumPy arrays to PyTorch tensors for your models (e.g., `torch.tensor(data, dtype=torch.float32)` for features, `torch.tensor(labels, dtype=torch.long)` for labels).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d75b2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define label mappings\n",
        "composer_to_label = {composer: i for i, composer in enumerate(COMPOSERS)}\n",
        "label_to_composer = {i: composer for composer, i in composer_to_label.items()}\n",
        "\n",
        "features_cnn = []\n",
        "features_lstm = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each composer\n",
        "for composer in COMPOSERS:\n",
        "    composer_dir = os.path.join(MIDI_DIR)\n",
        "    print(f\"Processing composer: {composer}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            print(os.path.join(root, file))\n",
        "            # Check if the file is a MIDI file and contains 'bach' in its name.\n",
        "            # There are other composers that need to be processed too.\n",
        "            if (file.endswith('.mid') or file.endswith('.midi')) and composer.lower() in file.lower():\n",
        "                midi_path = os.path.join(root, file)\n",
        "                print(\"Reading file: \", file)\n",
        "\n",
        "                try:\n",
        "                    segments = extract_segments_from_midi(midi_path, SEGMENT_DURATION_SECONDS, SAMPLES_PER_SECOND)\n",
        "                except Exception as e:\n",
        "                    print(f\"Skipping {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                for segment in segments:\n",
        "                    all_augmented = [segment]\n",
        "\n",
        "                    for step in AUGMENT_TRANSPOSITION_STEPS:\n",
        "                        all_augmented.append(apply_augmentation(segment, 'transpose', step))\n",
        "                    for scale in AUGMENT_TEMPO_SCALES:\n",
        "                        all_augmented.append(apply_augmentation(segment, 'tempo_scale', scale))\n",
        "\n",
        "                    for augmented_segment in all_augmented:\n",
        "                        # CNN Features\n",
        "                        piano_roll = midi_to_piano_roll(augmented_segment, duration=SEGMENT_DURATION_SECONDS,\n",
        "                                                        samples_per_second=SAMPLES_PER_SECOND,\n",
        "                                                        pitch_low=PITCH_LOW, pitch_high=PITCH_HIGH)\n",
        "                        if piano_roll is not None:\n",
        "                            features_cnn.append(piano_roll)\n",
        "\n",
        "                        # LSTM Features\n",
        "                        sequential = midi_to_sequential_features(augmented_segment, duration=SEGMENT_DURATION_SECONDS,\n",
        "                                                                 samples_per_second=SAMPLES_PER_SECOND,\n",
        "                                                                 pitch_low=PITCH_LOW, pitch_high=PITCH_HIGH)\n",
        "                        if sequential is not None:\n",
        "                            features_lstm.append(sequential)\n",
        "\n",
        "                        # Append label only if both features were generated\n",
        "                        if piano_roll is not None and sequential is not None:\n",
        "                            labels.append(composer_to_label[composer])\n",
        "\n",
        "print(\"Finished processing all composers.\")\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "features_cnn = np.array(features_cnn, dtype=np.float32)\n",
        "features_lstm = np.array(features_lstm, dtype=np.float32)\n",
        "labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "# Save to disk\n",
        "with open(os.path.join(OUTPUT_DIR, 'features_cnn.pkl'), 'wb') as f:\n",
        "    pickle.dump(features_cnn, f)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'features_lstm.pkl'), 'wb') as f:\n",
        "    pickle.dump(features_lstm, f)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'labels.pkl'), 'wb') as f:\n",
        "    pickle.dump(labels, f)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'composer_to_label.pkl'), 'wb') as f:\n",
        "    pickle.dump(composer_to_label, f)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'label_to_composer.pkl'), 'wb') as f:\n",
        "    pickle.dump(label_to_composer, f)\n",
        "\n",
        "print(f\"Saved {len(labels)} labeled examples for training.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8943da3f",
      "metadata": {},
      "source": [
        "CNN Input: (batch_size, 1, 88, 500) → channel-first PyTorch format (grayscale piano roll)\n",
        "CNN Output per segment: (batch_size, time_steps=some_N, features_per_step=some_M)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938670e2",
      "metadata": {
        "id": "938670e2"
      },
      "outputs": [],
      "source": [
        "class ComposerCNN(nn.Module):\n",
        "    def __init__(self, num_pitches, num_time_steps):\n",
        "        super(ComposerCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)\n",
        "        self.fc1 = nn.Linear(64 * num_pitches * (num_time_steps // 4), 128)\n",
        "        self.fc2 = nn.Linear(128, len(COMPOSERS))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, (2, 2))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, (2, 2))\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53aeb56d",
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cnn = ComposerCNN(NUM_PITCHES, int(SEGMENT_DURATION_SECONDS * SAMPLES_PER_SECOND))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9514fe3c",
      "metadata": {
        "id": "9514fe3c"
      },
      "source": [
        "Model Building: Develop a deep learning model using LSTM and CNN architectures to classify the musical scores according to the composer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d17183",
      "metadata": {},
      "source": [
        "LSTM Input Shape: (batch_size, time_steps, features_per_step) → same as (batch_size, seq_len, input_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce65db9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define label mappings\n",
        "composer_to_label = {composer: i for i, composer in enumerate(COMPOSERS)}\n",
        "label_to_composer = {i: composer for composer, i in composer_to_label.items()}\n",
        "\n",
        "features_cnn = []\n",
        "features_lstm = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each composer\n",
        "for composer in COMPOSERS:\n",
        "    composer_dir = os.path.join(MIDI_DIR)\n",
        "    print(f\"Processing composer: {composer}\")\n",
        "\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            print(os.path.join(root, file))\n",
        "            # Check if the file is a MIDI file and contains 'bach' in its name.\n",
        "            # There are other composers that need to be processed too.\n",
        "            if (file.endswith('.mid') or file.endswith('.midi')) and composer.lower() in file.lower():\n",
        "                print(\"Reading file: \", file)\n",
        "                midi_path = os.path.join(root, file)\n",
        "\n",
        "                midi_path = os.path.join(root, file)\n",
        "\n",
        "                try:\n",
        "                    segments = extract_segments_from_midi(midi_path, SEGMENT_DURATION_SECONDS, SAMPLES_PER_SECOND)\n",
        "                except Exception as e:\n",
        "                    print(f\"Skipping {file}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                for segment in segments:\n",
        "                    all_augmented = [segment]\n",
        "\n",
        "                    for step in AUGMENT_TRANSPOSITION_STEPS:\n",
        "                        all_augmented.append(apply_augmentation(segment, 'transpose', step))\n",
        "                    for scale in AUGMENT_TEMPO_SCALES:\n",
        "                        all_augmented.append(apply_augmentation(segment, 'tempo_scale', scale))\n",
        "\n",
        "                    for augmented_segment in all_augmented:\n",
        "                        # CNN Features\n",
        "                        piano_roll = midi_to_piano_roll(augmented_segment, duration=SEGMENT_DURATION_SECONDS,\n",
        "                                                        samples_per_second=SAMPLES_PER_SECOND,\n",
        "                                                        pitch_low=PITCH_LOW, pitch_high=PITCH_HIGH)\n",
        "                        if piano_roll is not None:\n",
        "                            features_cnn.append(piano_roll)\n",
        "\n",
        "                        # LSTM Features\n",
        "                        sequential = midi_to_sequential_features(augmented_segment, duration=SEGMENT_DURATION_SECONDS,\n",
        "                                                                 samples_per_second=SAMPLES_PER_SECOND,\n",
        "                                                                 pitch_low=PITCH_LOW, pitch_high=PITCH_HIGH)\n",
        "                        if sequential is not None:\n",
        "                            features_lstm.append(sequential)\n",
        "\n",
        "                        # Append label only if both features were generated\n",
        "                        if piano_roll is not None and sequential is not None:\n",
        "                            labels.append(composer_to_label[composer])\n",
        "\n",
        "print(\"Finished processing all composers.\")\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "features_cnn = np.array(features_cnn, dtype=np.float32)\n",
        "features_lstm = np.array(features_lstm, dtype=np.float32)\n",
        "labels = np.array(labels, dtype=np.int64)\n",
        "\n",
        "# Save to disk\n",
        "with open(os.path.join(OUTPUT_DIR, 'features_cnn.pkl'), 'wb') as f:\n",
        "    pickle.dump(features_cnn, f)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'features_lstm.pkl'), 'wb') as f:\n",
        "    pickle.dump(features_lstm, f)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'labels.pkl'), 'wb') as f:\n",
        "    pickle.dump(labels, f)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'composer_to_label.pkl'), 'wb') as f:\n",
        "    pickle.dump(composer_to_label, f)\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'label_to_composer.pkl'), 'wb') as f:\n",
        "    pickle.dump(label_to_composer, f)\n",
        "\n",
        "print(f\"Saved {len(labels)} labeled examples for training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba1b6969",
      "metadata": {
        "id": "ba1b6969"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 13         # 12 chroma + 1 note density\n",
        "hidden_size = 128       # Can be tuned\n",
        "num_layers = 2          # Can be tuned\n",
        "num_classes = 10        # Update based on your label count\n",
        "batch_size = 64\n",
        "num_epochs = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# ------------------------------\n",
        "# Load Preprocessed Data\n",
        "# ------------------------------\n",
        "with open('/content/processed_data/features_lstm.pkl', 'rb') as f:\n",
        "    X = pickle.load(f)\n",
        "with open('/content/processed_data/labels.pkl', 'rb') as f:\n",
        "    y = pickle.load(f)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)        # Shape: (N, 500, 13)\n",
        "y_tensor = torch.tensor(y, dtype=torch.long)           # Shape: (N,)\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "\n",
        "# ------------------------------\n",
        "# Define the LSTM Model\n",
        "# ------------------------------\n",
        "class ComposerLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(ComposerLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=input_size,\n",
        "                            hidden_size=hidden_size,\n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True,\n",
        "                            dropout=0.3,\n",
        "                            bidirectional=False)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, input_size)\n",
        "        lstm_out, _ = self.lstm(x)  # output: (batch_size, seq_len, hidden_size)\n",
        "        out = lstm_out[:, -1, :]    # Take last time step\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ComposerLSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e26b825f",
      "metadata": {
        "id": "e26b825f"
      },
      "source": [
        "Model Training: Train the deep learning model using the pre-processed and feature-extracted data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f15c65da",
      "metadata": {
        "id": "f15c65da"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# Training Loop\n",
        "# ------------------------------\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_val, y_val in val_loader:\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "            outputs = model(X_val)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += y_val.size(0)\n",
        "            correct += (predicted == y_val).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"composer_lstm_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c51863c3",
      "metadata": {
        "id": "c51863c3"
      },
      "source": [
        "Model Evaluation: Evaluate the performance of the deep learning model using accuracy, precision, and recall metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad55a6b9",
      "metadata": {
        "id": "ad55a6b9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "63a7f955",
      "metadata": {
        "id": "63a7f955"
      },
      "source": [
        "Model Optimization: Optimize the deep learning model by fine-tuning hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "180e4eec",
      "metadata": {
        "id": "180e4eec"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aai-511-finalproject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
